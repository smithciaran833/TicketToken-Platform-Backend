# Event Service External/Infrastructure Services Analysis
## Purpose: Integration Testing Documentation
## Source: blockchain.service.ts, venue-service.client.ts, cache-integration.ts, databaseService.ts, healthCheck.service.ts, reservation-cleanup.service.ts
## Generated: January 20, 2026

---

## EXECUTIVE SUMMARY

This document analyzes the external/infrastructure integration layer of the event-service. These files handle connections to external systems (blockchain, databases, HTTP services) and provide resilience patterns for the core service layer.

**Key Findings:**
- ‚úÖ **Gold Standard**: `venue-service.client.ts` - Excellent circuit breaker, retry, and fallback implementation
- üî¥ **Critical Gap**: `cache-integration.ts` - Missing tenant isolation in Redis keys
- üî¥ **Critical Gap**: `blockchain.service.ts` - No tenant validation before blockchain operations
- ‚ö†Ô∏è **High Priority**: Most services missing circuit breakers and timeouts

---

## ORPHANED FILES (Not Analyzed)

**None detected** - All 6 files are imported and actively used by the event-service.

---

## FILE ANALYSIS

---

### üìÑ blockchain.service.ts

**Location**: `src/services/blockchain.service.ts`

#### PURPOSE
- Wraps shared `@tickettoken/shared` BlockchainClient with event-specific logic
- Creates immutable on-chain event accounts on Solana blockchain
- Manages royalty splits (artist/venue percentages) stored on-chain
- **Core Dependencies**: Event creation flows (`event.service.ts`) depend on this for blockchain registration

#### EXTERNAL CONNECTIONS
| Type | Endpoint/Resource | Configuration |
|------|------------------|---------------|
| Solana RPC | `SOLANA_RPC_URL` | Default: https://api.devnet.solana.com |
| Program ID | `TICKETTOKEN_PROGRAM_ID` | Default: BnYanHjkV6bBDFYfC7F76TyYk6NA9p3wvcAfY1XZCXYS |
| Wallet File | `PLATFORM_WALLET_PATH` | Filesystem read for signing transactions |
| No HTTP endpoints | Uses Solana web3.js SDK directly | Connection via RPC only |

#### RESILIENCE PATTERNS
‚ö†Ô∏è **CRITICAL GAPS - NO RESILIENCE**:
- ‚ùå **NO circuit breaker** for blockchain calls
- ‚ùå **NO retry logic** implemented
- ‚ùå **NO timeout configuration** on blockchain operations
- ‚ùå **NO fallback behavior** when blockchain unavailable
- ‚úÖ Lazy initialization (client created on first use only)
- ‚ùå No differentiation between retryable vs non-retryable blockchain errors

**Impact**: Blockchain failures could hang indefinitely or cascade to event creation failures.

#### ERROR HANDLING
| Pattern | Status | Details |
|---------|--------|---------|
| Custom Errors | ‚úÖ | Throws `BlockchainError` for consistent error handling |
| Logging | ‚úÖ | Detailed pino logging with eventId, venueId, error context |
| Validation | ‚úÖ | Validates total royalty ‚â§ 100% before blockchain call |
| Error Recovery | ‚ùå | No retry or fallback on blockchain failures |
| Error Classification | ‚ùå | Doesn't distinguish transient vs permanent failures |

#### TENANT ISOLATION
üî¥ **CRITICAL SECURITY ISSUE**:
- ‚ùå **NO tenant_id passed through** to blockchain operations
- ‚ùå No validation that venue belongs to tenant before creating on-chain event
- ‚ùå Could create events for venues outside tenant's scope
- ‚ùå Blockchain event PDA uses only `venueId`, not tenant-scoped

**Risk**: Tenant A could create blockchain events for Tenant B's venues if validation is bypassed upstream.

#### POTENTIAL ISSUES
| Severity | Issue | Impact |
|----------|-------|--------|
| üî¥ CRITICAL | Missing tenant validation before blockchain operations | Data isolation breach |
| üî¥ CRITICAL | No tenant_id in blockchain event accounts | Cross-tenant event creation possible |
| ‚ö†Ô∏è HIGH | No timeout on blockchain transactions | Operations could hang indefinitely |
| ‚ö†Ô∏è HIGH | No retry logic for transient network failures | Reduced reliability |
| ‚ö†Ô∏è HIGH | No circuit breaker | Cascading failures when Solana RPC is down |
| üü° MEDIUM | Reads wallet from filesystem without error handling | Startup failures possible |
| üü° MEDIUM | No validation that venue exists before blockchain creation | Orphaned on-chain accounts |

---

### üìÑ venue-service.client.ts

**Location**: `src/services/venue-service.client.ts`

#### PURPOSE
- Service-to-service client for venue-service HTTP API
- Validates venue access and retrieves venue details for event creation
- **Core Dependencies**: Event creation/validation flows depend on venue existence checks
- **Key Operations**: `validateVenueAccess()`, `getVenue()`, `healthCheck()`

#### EXTERNAL CONNECTIONS
| Type | Endpoint/Resource | Configuration |
|------|------------------|---------------|
| HTTP/HTTPS | `VENUE_SERVICE_URL` | Default: http://venue-service:3002 |
| HTTPS Enforcement | Converts HTTP‚ÜíHTTPS in production | Override: `ALLOW_INSECURE_SERVICE_CALLS=true` |
| S2S Authentication | Service credentials via `getS2SHeaders()` | JWT-based service identity |
| Tenant Context | Passes `X-Tenant-ID` header | Ensures RLS enforcement |

#### RESILIENCE PATTERNS
‚úÖ **GOLD STANDARD IMPLEMENTATION**:
- ‚úÖ **Circuit breaker** (opossum) with intelligent thresholds:
  - Timeout: 5000ms per request
  - Error threshold: 50% failure rate
  - Reset timeout: 30s (half-open state)
  - Volume threshold: 5 requests minimum
- ‚úÖ **Retry logic** with exponential backoff:
  - Max retries: 3
  - Initial delay: 500ms
  - Max delay: 5000ms
  - Smart retry: Doesn't retry 4xx errors (except 429)
- ‚úÖ **Fallback behavior**:
  - In-memory cache with tenant-aware keys
  - Degraded mode allows operations with cached/default data
  - Prevents total service failure
- ‚úÖ **Idempotency keys** for mutating operations:
  - Format: `event-svc:{operation}:{resourceId}:{timestamp}:{nonce}`
  - Prevents duplicate operations on retry
- ‚úÖ **Circuit breaker events** logged (open/close/halfOpen/fallback)

#### ERROR HANDLING
| Pattern | Status | Details |
|---------|--------|---------|
| Custom Errors | ‚úÖ | `NotFoundError`, `ForbiddenError`, `ValidationError` |
| Status Code Handling | ‚úÖ | Proper differentiation (404, 403, 429, 5xx) |
| Retry Strategy | ‚úÖ | Doesn't retry 4xx (except 429), retries 5xx/network errors |
| Logging | ‚úÖ | Detailed context: venueId, tenantId, status, error message |
| Error Propagation | ‚úÖ | Converts HTTP errors to domain-specific exceptions |

#### TENANT ISOLATION
‚úÖ **EXCELLENT TENANT-AWARE DESIGN**:
- ‚úÖ Tenant-aware caching: `getCacheKey(tenantId, venueId)` ‚Üí `${tenantId}:${venueId}`
- ‚úÖ Passes `X-Tenant-ID` header on all HTTP requests
- ‚úÖ Cache invalidation respects tenant boundaries
- ‚úÖ Fallback responses maintain tenant context
- ‚úÖ Prevents cross-tenant data leakage in cache

**Cache Example:**
```typescript
// Cache key format ensures tenant isolation
function getCacheKey(tenantId: string, venueId: string): string {
  return `${tenantId}:${venueId}`;
}
```

#### POTENTIAL ISSUES
| Severity | Issue | Impact |
|----------|-------|--------|
| üü° MEDIUM | In-memory cache doesn't scale across instances | Cache misses on different pods |
| üü° MEDIUM | Degraded mode allows operations without venue verification | Security trade-off for availability |
| üü° MEDIUM | HTTPS enforcement can be bypassed with env flag | Production security concern |
| üü¢ LOW | Overall excellent implementation | Minimal risk |

**Recommendation**: Migrate in-memory cache to Redis with tenant prefixing for multi-instance deployments.

---

### üìÑ cache-integration.ts

**Location**: `src/services/cache-integration.ts`

#### PURPOSE
- Redis wrapper for caching service data
- Provides `get()`, `set()`, `delete()`, `invalidateCache()`, `flush()` operations
- **Core Dependencies**: Used throughout event-service for performance optimization
- **Export**: Singleton `serviceCache` instance

#### EXTERNAL CONNECTIONS
| Type | Endpoint/Resource | Configuration |
|------|------------------|---------------|
| Redis | `REDIS_HOST:REDIS_PORT` | Default: localhost:6379 |
| Password | `REDIS_PASSWORD` | Optional authentication |
| Connection | Single Redis client | No pooling configured |

#### RESILIENCE PATTERNS
‚ö†Ô∏è **PARTIAL IMPLEMENTATION**:
- ‚úÖ **Retry strategy**: Exponential backoff (50ms √ó attempt, max 2000ms)
- ‚úÖ **Max retries per request**: 3
- ‚úÖ **Graceful degradation**: Returns `null` on errors (doesn't throw)
- ‚ùå **NO timeout configured** - Operations could hang indefinitely
- ‚ùå **NO circuit breaker** for Redis failures
- ‚ùå No connection pooling
- ‚ùå No monitoring/metrics

**Impact**: Redis failures won't crash the app but could cause performance degradation.

#### ERROR HANDLING
| Pattern | Status | Details |
|---------|--------|---------|
| Try-Catch Blocks | ‚úÖ | All operations wrapped in error handling |
| Error Logging | ‚úÖ | Logs with context (key, TTL, error) |
| Silent Failures | ‚úÖ | Returns `null` instead of throwing |
| Error Metrics | ‚ùå | No metrics/alerting for Redis failures |

#### TENANT ISOLATION
üî¥ **CRITICAL SECURITY VULNERABILITY**:
- ‚ùå **NO tenant prefixing in cache keys**
- ‚ùå Cache keys are NOT tenant-aware
- ‚ùå `flush()` would clear ALL tenants' data
- ‚ùå Pattern-based deletion (`keys *`) could affect all tenants
- ‚ùå No tenant scoping in any method

**SECURITY RISK**: Tenant A could potentially access Tenant B's cached data if cache keys collide.

**Example Vulnerability:**
```typescript
// CURRENT (VULNERABLE):
await serviceCache.set('event:123', eventData); // No tenant context!

// SHOULD BE:
await serviceCache.set(`tenant:${tenantId}:event:123`, eventData);
```

#### POTENTIAL ISSUES
| Severity | Issue | Impact |
|----------|-------|--------|
| üî¥ CRITICAL | Missing tenant isolation in Redis keys | Cross-tenant data leakage |
| üî¥ CRITICAL | `flush()` method is dangerous | Clears all tenants' data |
| üî¥ CRITICAL | Wildcard pattern matching (`keys *`) dangerous | Affects all tenants |
| ‚ö†Ô∏è HIGH | No timeout on Redis operations | Operations could hang |
| ‚ö†Ô∏è HIGH | No circuit breaker for Redis failures | Cascading failures possible |
| ‚ö†Ô∏è HIGH | Uses `keys` command (blocks Redis) | Performance issue in production |
| üü° MEDIUM | No connection pooling configuration | Scalability concern |
| üü° MEDIUM | No monitoring/alerting | Ops blind to Redis issues |

**REQUIRED FIX**: Add tenant prefixing to ALL cache operations immediately.

---

### üìÑ databaseService.ts

**Location**: `src/services/databaseService.ts`

#### PURPOSE
- PostgreSQL connection pool manager
- Foundation for ALL database operations in event-service
- **Core Dependencies**: Used by all repositories/models for database access
- **Key Operations**: `initialize()`, `getPool()`

#### EXTERNAL CONNECTIONS
| Type | Endpoint/Resource | Configuration |
|------|------------------|---------------|
| PostgreSQL | `DATABASE_URL` or individual params | Default: tickettoken-postgres:5432 |
| Database | `DB_NAME` | Default: tickettoken_db |
| Credentials | `DB_USER`, `DB_PASSWORD` | Default: postgres/localdev123 |

#### RESILIENCE PATTERNS
‚ùå **NO RESILIENCE IMPLEMENTED**:
- ‚ùå **NO retry logic** on connection failures
- ‚ùå **NO circuit breaker**
- ‚ùå **NO timeout configuration**
- ‚ùå **NO connection pool limits** configured (uses pg defaults)
- ‚ùå **NO health checks** or connection validation
- ‚ùå **NO fallback behavior**
- ‚ùå **NO connection retry on disconnect**

**Impact**: Database connection failures will immediately crash queries with no recovery.

#### ERROR HANDLING
| Pattern | Status | Details |
|---------|--------|---------|
| Initialization Check | ‚úÖ | Throws error if pool not initialized |
| Connection Test | ‚úÖ | `SELECT NOW()` on initialization |
| Error Recovery | ‚ùå | No retry or reconnection logic |
| Graceful Degradation | ‚ùå | No fallback behavior |
| Connection Monitoring | ‚ùå | No health checks after initialization |

#### TENANT ISOLATION
‚úÖ **RLS-Based Isolation**:
- ‚ÑπÔ∏è Database uses Row Level Security (RLS) per platform architecture
- ‚ÑπÔ∏è This service doesn't enforce tenant context (relies on RLS policies)
- ‚ÑπÔ∏è Tenant isolation handled at query level via `set_config('app.tenant_id', ...)`, not connection level
- ‚úÖ Proper design: Connection pool is tenant-agnostic

**Note**: This is correct architecture - tenant enforcement happens in query layer, not connection layer.

#### POTENTIAL ISSUES
| Severity | Issue | Impact |
|----------|-------|--------|
| ‚ö†Ô∏è HIGH | No connection pool configuration | Max connections, idle timeout not set |
| ‚ö†Ô∏è HIGH | No retry logic for failed connections | Service crashes on DB outage |
| ‚ö†Ô∏è HIGH | No circuit breaker for database failures | Cascading failures |
| ‚ö†Ô∏è HIGH | No timeout configuration | Queries could hang indefinitely |
| üü° MEDIUM | No connection health monitoring | Can't detect stale connections |
| üü° MEDIUM | No graceful shutdown handling | Potential connection leaks |
| üü° MEDIUM | Minimal error handling on initialization failure | Poor startup diagnostics |

**REQUIRED FIX**: Add connection pool limits, retry logic, and health monitoring.

---

### üìÑ healthCheck.service.ts

**Location**: `src/services/healthCheck.service.ts`

#### PURPOSE
- Kubernetes liveness, readiness, and startup probes
- Monitoring dashboard health checks
- **Core Dependencies**: Used by k8s orchestration and monitoring systems
- **Key Operations**: `performLivenessCheck()`, `performReadinessCheck()`, `performHealthCheck()`

#### EXTERNAL CONNECTIONS
| Type | Endpoint/Resource | Configuration |
|------|------------------|---------------|
| Database | Health check queries (`SELECT 1`) | Required for readiness |
| Redis | Ping checks | Required for readiness |
| External (Optional) | Venue-service, auth-service | Cached, non-critical checks |

#### RESILIENCE PATTERNS
‚úÖ **EXCELLENT ARCHITECTURE** (Audit fixes already applied):
- ‚úÖ **Timeouts**: DB 2s, Redis 1s (prevents hanging probes)
- ‚úÖ **Degraded state detection**: Slow response thresholds
  - DB > 1000ms = degraded
  - Redis > 500ms = degraded
- ‚úÖ **Prevents cascading failures**: External dependencies don't affect readiness
- ‚úÖ **Caching for external checks**: 30s TTL (reduces check frequency)
- ‚úÖ **Clock drift detection**: 5s tolerance (audit fix TSO-1)
- ‚úÖ **Fast liveness**: <100ms, no dependency checks

**Design Pattern:**
```typescript
// CRITICAL: External services don't affect health status
async performHealthCheck(db, redis, includeExternalDeps = false) {
  // Only LOCAL dependencies affect status
  const status = allLocalUp ? 'healthy' : 'unhealthy';
  
  // External deps are INFORMATIONAL ONLY
  if (includeExternalDeps) {
    result.dependencies = await this.checkExternalDependencies();
  }
}
```

#### ERROR HANDLING
| Pattern | Status | Details |
|---------|--------|---------|
| Try-Catch All Checks | ‚úÖ | No uncaught exceptions |
| Structured Error Codes | ‚úÖ | Returns codes, not internal messages |
| Status Levels | ‚úÖ | up/degraded/down (3-level status) |
| Logging | ‚úÖ | Detailed diagnostics without exposing secrets |
| Timeout Enforcement | ‚úÖ | All checks have timeouts |

#### TENANT ISOLATION
‚úÖ **PROPER DESIGN**:
- N/A - Health checks are service-level, not tenant-specific
- ‚úÖ No tenant data exposed in health responses
- ‚úÖ Properly scoped to infrastructure health

#### POTENTIAL ISSUES
| Severity | Issue | Impact |
|----------|-------|--------|
| üü° MEDIUM | `performDetailedHealthCheck()` includes sensitive info | Memory, PID exposed - should require admin auth |
| üü° MEDIUM | External service checks use `fetch` without circuit breaker | Could slow health checks |
| üü¢ LOW | Overall well-designed | Minimal risk |

**Note**: This is one of the best-designed files in the codebase.

---

### üìÑ reservation-cleanup.service.ts

**Location**: `src/services/reservation-cleanup.service.ts`

#### PURPOSE
- Background job to release expired ticket reservations
- Ensures capacity is freed when users don't complete purchases
- **Core Dependencies**: Depends on `CapacityService` for cleanup logic
- **Key Operations**: `start()`, `stop()`, `runCleanup()`, `triggerCleanup()`

#### EXTERNAL CONNECTIONS
| Type | Endpoint/Resource | Configuration |
|------|------------------|---------------|
| Database | Indirect via `CapacityService` (Knex) | Queries reservation tables |
| No HTTP calls | Internal service only | - |

#### RESILIENCE PATTERNS
‚ö†Ô∏è **MINIMAL RESILIENCE**:
- ‚ùå **NO timeout** on cleanup operations
- ‚ùå **NO circuit breaker**
- ‚úÖ Error handling prevents job from crashing
- ‚úÖ Runs on interval (configurable, default 1 minute)
- ‚ùå **NO retry logic** if cleanup fails
- ‚ùå **NO dead letter queue** for failed cleanups
- ‚ùå **NO backoff** if database is struggling

**Impact**: Background job continues hitting DB even if overwhelmed or down.

#### ERROR HANDLING
| Pattern | Status | Details |
|---------|--------|---------|
| Try-Catch Around Cleanup | ‚úÖ | Prevents job crash |
| Logs Errors | ‚úÖ | Error + stack trace logged |
| Continues After Errors | ‚úÖ | Job keeps running |
| Alerting on Failures | ‚ùå | No alerting for repeated failures |
| Circuit Breaker | ‚ùå | Doesn't stop if DB is down |

#### TENANT ISOLATION
‚ö†Ô∏è **UNCLEAR - DEPENDS ON CAPACITYSERVICE**:
- ‚ö†Ô∏è This file doesn't show tenant filtering logic
- ‚ö†Ô∏è **Need to verify**: Does `CapacityService.releaseExpiredReservations()` include tenant scoping?
- üî¥ **FLAG**: If CapacityService doesn't filter by tenant, this could release ALL tenants' reservations

**Required Action**: Audit `CapacityService` implementation to verify tenant isolation.

#### POTENTIAL ISSUES
| Severity | Issue | Impact |
|----------|-------|--------|
| üî¥ CRITICAL | Tenant isolation unclear | Need to verify CapacityService |
| ‚ö†Ô∏è HIGH | No timeout on cleanup operations | Could block indefinitely |
| ‚ö†Ô∏è HIGH | No circuit breaker | Continues hitting DB even if down |
| ‚ö†Ô∏è HIGH | No monitoring/alerting for failed cleanups | Ops blind to failures |
| üü° MEDIUM | Fixed 1-minute interval | May be too aggressive for large datasets |
| üü° MEDIUM | No graceful handling if cleanup > interval | Overlapping cleanups possible |
| üü° MEDIUM | No metrics on cleanup performance | Can't detect degradation |

**REQUIRED ACTION**: 
1. Verify tenant isolation in CapacityService
2. Add circuit breaker to stop job if DB is down
3. Add alerting for repeated cleanup failures

---

## GOLD STANDARD

### üèÜ venue-service.client.ts - Reference Implementation

This file demonstrates **excellent resilience patterns** and should be used as a template for other external integrations:

#### What Makes It Gold Standard

1. **Circuit Breaker (opossum)**
   - Prevents cascading failures
   - Configurable thresholds (50% error rate, 5 request minimum)
   - Automatic recovery with half-open state
   - Event logging for observability

2. **Retry Logic with Exponential Backoff**
   - Max 3 retries with increasing delays (500ms ‚Üí 5000ms)
   - Smart retry: Doesn't retry 4xx errors (except 429)
   - Prevents thundering herd

3. **Fallback Behavior**
   - In-memory cache for degraded mode
   - Returns cached/default data when service unavailable
   - Graceful degradation instead of hard failures

4. **Idempotency Keys**
   - Prevents duplicate operations on retry
   - Format: `event-svc:{operation}:{resourceId}:{timestamp}:{nonce}`
   - Only added for mutating operations (POST/PUT/PATCH/DELETE)

5. **Tenant Isolation**
   - Tenant-aware cache keys: `${tenantId}:${venueId}`
   - Passes `X-Tenant-ID` header on all requests
   - Prevents cross-tenant data leakage

6. **HTTPS Enforcement**
   - Converts HTTP ‚Üí HTTPS in production
   - Configurable override for development

7. **Comprehensive Error Handling**
   - Status code differentiation (404, 403, 429, 5xx)
   - Custom domain exceptions
   - Detailed logging with context

#### Code Example to Replicate

```typescript
// Circuit breaker setup
this.circuitBreaker = new CircuitBreaker(this.requestWithRetry.bind(this), {
  timeout: 5000,
  errorThresholdPercentage: 50,
  resetTimeout: 30000,
  volumeThreshold: 5,
});

// Retry with exponential backoff
private async requestWithRetry(path: string, options: any = {}): Promise<any> {
  return withRetry(
    () => this.request(path, options),
    {
      maxRetries: 3,
      initialDelayMs: 500,
      maxDelayMs: 5000,
      retryOn: (error) => {
        if (error.status >= 400 && error.status < 500 && error.status !== 429) {
          return false; // Don't retry 4xx (except 429)
        }
        return isRetryableError(error);
      },
    }
  );
}

// Tenant-aware caching
function getCacheKey(tenantId: string, venueId: string): string {
  return `${tenantId}:${venueId}`;
}

// Fallback behavior
if (this.isDegraded || error.message?.includes('Breaker is open')) {
  const cached = this.getCachedVenue(tenantId, venueId);
  if (cached) {
    return cached; // Use cache when service down
  }
  return defaultResponse; // Or return safe default
}
```

#### Apply This Pattern To

- ‚ùå `blockchain.service.ts` - Needs circuit breaker + retry
- ‚ùå `cache-integration.ts` - Needs circuit breaker + timeouts
- ‚ùå `databaseService.ts` - Needs retry + connection management

---

## CROSS-SERVICE DEPENDENCIES

### Dependency Map

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Event Service      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Solana Blockchain (RPC)
           ‚îÇ               ‚Ä¢ Program: tickettoken-program
           ‚îÇ               ‚Ä¢ No resilience patterns
           ‚îÇ               üî¥ Missing: Circuit breaker, timeout, retry
           ‚îÇ
           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Venue Service (HTTP)
           ‚îÇ               ‚Ä¢ S2S authenticated calls
           ‚îÇ               ‚úÖ Circuit breaker + retry + fallback
           ‚îÇ               ‚úÖ Tenant-aware caching
           ‚îÇ
           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ PostgreSQL Database
           ‚îÇ               ‚Ä¢ Connection pool via pg
           ‚îÇ               üî¥ Missing: Retry, circuit breaker, timeouts
           ‚îÇ               ‚úÖ RLS for tenant isolation
           ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Redis Cache
                           ‚Ä¢ Single connection
                           üî¥ Missing: Tenant prefixing in keys
                           üî¥ Missing: Circuit breaker, timeout
```

### External Service Inventory

| Service | Type | Auth | Resilience | Tenant-Aware | Status |
|---------|------|------|------------|--------------|--------|
| Solana RPC | Blockchain | None | ‚ùå None | ‚ùå No | üî¥ Critical gaps |
| Venue Service | HTTP/S2S | S2S JWT | ‚úÖ Full | ‚úÖ Yes | ‚úÖ Gold standard |
| PostgreSQL | Database | Password | ‚ùå None | ‚úÖ RLS | üî¥ Critical gaps |
| Redis | Cache | Optional password | ‚ö†Ô∏è Partial | ‚ùå No | üî¥ Critical gaps |

### Service Call Flows

#### Event Creation Flow
```
POST /api/v1/events
  ‚îÇ
  ‚îú‚îÄ‚ñ∫ venue-service.client.validateVenueAccess()
  ‚îÇ   ‚îî‚îÄ‚ñ∫ HTTP GET /api/v1/venues/{id}
  ‚îÇ       ‚úÖ Circuit breaker + retry + cache fallback
  ‚îÇ
  ‚îú‚îÄ‚ñ∫ databaseService.getPool().query()
  ‚îÇ   ‚îî‚îÄ‚ñ∫ PostgreSQL INSERT into events table
  ‚îÇ       üî¥ No retry, no circuit breaker
  ‚îÇ
  ‚îú‚îÄ‚ñ∫ blockchain.service.createEventOnChain()
  ‚îÇ   ‚îî‚îÄ‚ñ∫ Solana RPC: create_event instruction
  ‚îÇ       üî¥ No retry, no timeout, no circuit breaker
  ‚îÇ
  ‚îî‚îÄ‚ñ∫ cache-integration.delete()
      ‚îî‚îÄ‚ñ∫ Redis DEL event:*
          üî¥ No tenant prefixing
```

---

## RESILIENCE GAPS

### Summary by Priority

#### üî¥ CRITICAL (P0) - Security & Data Isolation

| File | Gap | Impact | Fix Required |
|------|-----|--------|--------------|
| `cache-integration.ts` | No tenant prefixing in Redis keys | Cross-tenant data leakage | Add `tenant:${id}:` prefix to ALL keys |
| `cache-integration.ts` | `flush()` clears all tenants | Data loss across tenants | Scope to tenant or remove method |
| `blockchain.service.ts` | No tenant validation | Cross-tenant event creation | Validate venue belongs to tenant |
| `reservation-cleanup.service.ts` | Tenant isolation unclear | May affect all tenants | Verify CapacityService scoping |

#### ‚ö†Ô∏è HIGH (P1) - Missing Resilience

| File | Missing Pattern | Impact | Fix Required |
|------|----------------|--------|--------------|
| `blockchain.service.ts` | No circuit breaker | Cascading failures on Solana outage | Add opossum circuit breaker |
| `blockchain.service.ts` | No timeout | Operations hang indefinitely | Add 10s timeout to transactions |
| `blockchain.service.ts` | No retry logic | Single-point failures | Add retry with exponential backoff |
| `databaseService.ts` | No connection pool limits | Connection exhaustion | Configure max connections, idle timeout |
| `databaseService.ts` | No retry on connection failure | Service crash on DB restart | Add connection retry logic |
| `databaseService.ts` | No circuit breaker | Cascading failures on DB outage | Add circuit breaker to connection pool |
| `cache-integration.ts` | No timeout | Redis operations hang | Add 1s timeout |
| `cache-integration.ts` | No circuit breaker | Cascading failures on Redis outage | Add circuit breaker |
| `reservation-cleanup.service.ts` | No circuit breaker | Job continues hitting down DB | Add circuit breaker to stop job |

#### üü° MEDIUM (P2) - Code Quality & Operations

| File | Gap | Impact | Fix Required |
|------|-----|--------|--------------|
| `cache-integration.ts` | Uses `keys *` command | Blocks Redis in production | Use SCAN instead |
| `databaseService.ts` | No graceful shutdown | Connection leaks on shutdown | Add graceful pool drain |
| `healthCheck.service.ts` | Detailed health needs auth | Info disclosure | Require admin middleware |
| `reservation-cleanup.service.ts` | No alerting | Blind to cleanup failures | Add metrics/alerting |
| `venue-service.client.ts` | In-memory cache | Doesn't scale across instances | Migrate to Redis with tenant prefix |

---

## INTEGRATION TEST FILE MAPPING

### Test Coverage Recommendations

| Service File | Test File (Proposed) | Priority | Key Scenarios |
|-------------|---------------------|----------|---------------|
| `blockchain.service.ts` | `tests/integration/blockchain-resilience.test.ts` | üî¥ P0 | ‚Ä¢ Solana RPC timeout<br>‚Ä¢ Network failures during tx<br>‚Ä¢ Tenant validation<br>‚Ä¢ Duplicate event creation<br>‚Ä¢ Royalty calculation |
| `venue-service.client.ts` | `tests/integration/venue-client-resilience.test.ts` | üü° P2 | ‚Ä¢ Circuit breaker behavior<br>‚Ä¢ Retry logic<br>‚Ä¢ Cache fallback<br>‚Ä¢ Degraded mode operations<br>‚Ä¢ Tenant isolation in cache<br>‚Ä¢ Idempotency key handling |
| `cache-integration.ts` | `tests/integration/cache-tenant-isolation.test.ts` | üî¥ P0 | ‚Ä¢ Tenant key isolation<br>‚Ä¢ Cross-tenant data leakage tests<br>‚Ä¢ Redis failover<br>‚Ä¢ Timeout handling<br>‚Ä¢ Pattern deletion safety |
| `databaseService.ts` | `tests/integration/database-pool-resilience.test.ts` | ‚ö†Ô∏è P1 | ‚Ä¢ Connection pool exhaustion<br>‚Ä¢ DB restart recovery<br>‚Ä¢ Connection retry logic<br>‚Ä¢ Graceful shutdown<br>‚Ä¢ RLS enforcement |
| `healthCheck.service.ts` | `tests/integration/health-check-isolation.test.ts` | üü° P2 | ‚Ä¢ External service failures don't affect readiness<br>‚Ä¢ Timeout enforcement<br>‚Ä¢ Degraded state detection<br>‚Ä¢ Clock drift detection |
| `reservation-cleanup.service.ts` | `tests/integration/cleanup-tenant-isolation.test.ts` | üî¥ P0 | ‚Ä¢ Tenant isolation in cleanup<br>‚Ä¢ DB failure handling<br>‚Ä¢ Overlapping job prevention<br>‚Ä¢ Metrics accuracy |

### Test Scenario Details

#### üî¥ P0: cache-tenant-isolation.test.ts
**Purpose**: Verify Redis cache cannot leak data across tenants

```typescript
describe('Cache Tenant Isolation', () => {
  it('should prevent cross-tenant cache key collision', async () => {
    // Tenant A sets data
    await serviceCache.set('event:123', { name: 'Tenant A Event' });
    
    // Tenant B should NOT see Tenant A's data
    const data = await serviceCache.get('event:123');
    // CURRENTLY FAILS - both tenants see same data
    // SHOULD FAIL - need tenant prefixing
  });
  
  it('should scope flush() to single tenant', async () => {
    // Setup: Multiple tenants have cached data
    // Action: flush() called
    // Expected: Only current tenant's cache cleared
    // CURRENTLY FAILS - flush() clears all tenants
  });
  
  it('should scope wildcard deletion to tenant', async () => {
    // Test that event:* doesn't delete other tenants' events
  });
});
```

#### üî¥ P0: blockchain-resilience.test.ts
**Purpose**: Verify blockchain operations are resilient and tenant-safe

```typescript
describe('Blockchain Resilience', () => {
  it('should timeout after 10s on hung RPC', async () => {
    // Mock Solana RPC that never responds
    // Should timeout, not hang forever
  });
  
  it('should retry transient network failures', async () => {
    // Mock RPC with 2 failures then success
    // Should succeed after retries
  });
  
  it('should prevent creating event for wrong tenant venue', async () => {
    // Tenant A tries to create event for Tenant B's venue
    // Should fail with ForbiddenError
    // CURRENTLY FAILS - no tenant check
  });
  
  it('should handle circuit breaker open state', async () => {
    // After multiple failures, circuit should open
    // Should fail fast without hitting RPC
  });
});
```

#### ‚ö†Ô∏è P1: database-pool-resilience.test.ts
**Purpose**: Verify database connection pool handles failures gracefully

```typescript
describe('Database Pool Resilience', () => {
  it('should retry connection on database restart', async () => {
    // Simulate DB restart during operation
    // Should retry and reconnect
    // CURRENTLY FAILS - no retry logic
  });
  
  it('should handle connection pool exhaustion', async () => {
    // Create max connections + 1
    // Should queue or fail gracefully
  });
  
  it('should enforce connection timeout', async () => {
    // Mock slow connection attempt
    // Should timeout after configured period
  });
});
```

#### üü° P2: venue-client-resilience.test.ts
**Purpose**: Verify venue-service.client resilience patterns work correctly

```typescript
describe('Venue Client Resilience', () => {
  it('should open circuit breaker after error threshold', async () => {
    // Make 5+ requests that fail
    // Circuit should open
    // Next request should fail fast
  });
  
  it('should use cached fallback when circuit open', async () => {
    // Prime cache with venue data
    // Open circuit breaker
    // Request should return cached data
  });
  
  it('should include idempotency key on mutating requests', async () => {
    // Make POST/PUT/PATCH/DELETE
    // Should include Idempotency-Key header
  });
  
  it('should NOT include idempotency key on GET requests', async () => {
    // Make GET request
    // Should NOT include Idempotency-Key header
  });
});
```

---

## RECOMMENDED FIXES

### Priority Order

#### 1. üî¥ P0: Fix Cache Tenant Isolation (IMMEDIATE)

**File**: `cache-integration.ts`

**Changes Required**:
```typescript
// Add tenant parameter to all methods
async get(tenantId: string, key: string): Promise<any> {
  const scopedKey = `tenant:${tenantId}:${key}`;
  // ... existing logic
}

async set(tenantId: string, key: string, value: any, ttl: number = 3600): Promise<void> {
  const scopedKey = `tenant:${tenantId}:${key}`;
  // ... existing logic
}

// REMOVE dangerous flush() method or scope to tenant
async flush(tenantId: string): Promise<void> {
  const pattern = `tenant:${tenantId}:*`;
  // Use SCAN instead of KEYS
  // ... safe deletion logic
}
```

**Breaking Change**: Yes - all callers must pass tenantId

#### 2. üî¥ P0: Add Blockchain Tenant Validation

**File**: `blockchain.service.ts`

**Changes Required**:
```typescript
async createEventOnChain(
  eventData: EventBlockchainData,
  tenantId: string // ADD THIS PARAMETER
): Promise<CreateEventResult> {
  // Validate venue belongs to tenant BEFORE blockchain call
  await this.validateVenueTenant(eventData.venueId, tenantId);
  
  // ... existing logic
}

private async validateVenueTenant(venueId: string, tenantId: string): Promise<void> {
  // Call venue-service or check database
  // Throw ForbiddenError if venue not in tenant
}
```

#### 3. ‚ö†Ô∏è P1: Add Blockchain Circuit Breaker

**File**: `blockchain.service.ts`

**Changes Required**:
```typescript
import CircuitBreaker from 'opossum';

export class EventBlockchainService {
  private circuitBreaker: CircuitBreaker;
  
  constructor() {
    this.circuitBreaker = new CircuitBreaker(
      this.createEventOnChainInternal.bind(this),
      {
        timeout: 10000, // 10s for blockchain operations
        errorThresholdPercentage: 50,
        resetTimeout: 60000, // 1 minute
      }
    );
  }
  
  async createEventOnChain(data: EventBlockchainData): Promise<CreateEventResult> {
    return this.circuitBreaker.fire(data);
  }
}
```

#### 4. ‚ö†Ô∏è P1: Add Database Connection Resilience

**File**: `databaseService.ts`

**Changes Required**:
```typescript
async initialize(): Promise<void> {
  const poolConfig = {
    // ... existing config
    max: 20, // Maximum connections
    idleTimeoutMillis: 30000,
    connectionTimeoutMillis: 5000,
  };
  
  // Add retry logic
  await withRetry(
    async () => {
      this.pool = new Pool(poolConfig);
      await this.pool.query('SELECT NOW()');
    },
    {
      maxRetries: 5,
      initialDelayMs: 1000,
      maxDelayMs: 10000,
    }
  );
}
```

---

## TESTING STRATEGY

### Integration Test Priorities

1. **P0 - Security Tests** (Must have before production)
   - Cache tenant isolation
   - Blockchain tenant validation
   - Reservation cleanup tenant scoping

2. **P1 - Resilience Tests** (Critical for reliability)
   - Circuit breaker behavior (all services)
   - Timeout enforcement
   - Retry logic validation
   - Fallback behavior

3. **P2 - Operations Tests** (Important for monitoring)
   - Health check isolation
   - Degraded mode operations
   - Metrics accuracy

### Test Environment Setup

```typescript
// Integration test setup for external services
beforeAll(async () => {
  // Start test containers
  await startRedis(); // Testcontainers
  await startPostgres(); // Testcontainers
  await startMockSolana(); // Mock RPC server
  await startMockVenueService(); // Mock HTTP server
});

// Tenant isolation test helper
async function withTenant<T>(tenantId: string, fn: () => Promise<T>): Promise<T> {
  // Set tenant context for test
  // Execute test function
  // Clean up tenant data
}
```

---

## METRICS & MONITORING

### Recommended Metrics

#### Circuit Breaker Metrics
```typescript
// Track circuit breaker state changes
metrics.gauge('circuit_breaker.state', state, { service: 'venue-service' });
metrics.counter('circuit_breaker.opened', { service: 'venue-service' });
metrics.counter('circuit_breaker.closed', { service: 'venue-service' });
```

#### External Service Metrics
```typescript
// Latency
metrics.histogram('external_service.latency', latencyMs, { 
  service: 'venue-service',
  operation: 'getVenue',
  status: 'success'
});

// Error rates
metrics.counter('external_service.errors', { 
  service: 'venue-service',
  error_type: 'timeout'
});

// Cache hit rates
metrics.counter('cache.hits', { tenant: tenantId });
metrics.counter('cache.misses', { tenant: tenantId });
```

---

## CONCLUSION

### Summary

- ‚úÖ **1 Gold Standard File**: `venue-service.client.ts` - Use as reference
- üî¥ **3 Critical Issues**: Cache tenant isolation, blockchain tenant validation, unclear cleanup scoping
- ‚ö†Ô∏è **8 High Priority Gaps**: Missing circuit breakers, timeouts, retry logic
- üü° **6 Medium Priority Issues**: Code quality and operational concerns

### Next Steps

1. **Immediate** (Sprint 1): Fix cache tenant isolation
2. **Critical** (Sprint 1): Add blockchain tenant validation
3. **High Priority** (Sprint 2): Add circuit breakers to blockchain + database
4. **Integration Tests** (Sprint 2-3): Implement P0 and P1 test scenarios
5. **Monitoring** (Sprint 3): Add metrics for all external services

### Architecture Recommendations

1. **Standardize Resilience**: Apply venue-service.client.ts patterns to all external integrations
2. **Tenant-First Design**: All cache/storage operations must include tenant context
3. **Circuit Breaker Strategy**: Add circuit breakers to ALL external dependencies
4. **Timeout Policy**: Enforce timeouts on all I/O operations (network, database, cache)
5. **Graceful Degradation**: Design fallback behavior for non-critical operations

---

**End of Analysis**
